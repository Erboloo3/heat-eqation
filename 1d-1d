#include <iostream>
#include <vector>
#include <cmath>
#include <iomanip>
#include <mpi.h>
#include <chrono>
#include <algorithm>
#include <fstream>

using namespace std;
using namespace std::chrono;

constexpr double PI = 3.14159265358979323846;

inline double exact_solution(double x, double t) {
    double sum = 0.0;
    for (int n = 1; n <= 1; ++n) {
        sum += (1.0 / pow(n * PI, 3)) * sin(n * PI * x) * exp(-pow(n * PI, 2) * t);
    }
    return 8 * sum;
}

void print_table(const vector<vector<double>>& table, const string& title, int rank) {
    cout << "\nProcess " << rank << " - " << title << ":\n";
    cout << "--------------------------------------------------\n";
    int rows = min((int)table.size(), 10);
    for (int i = 0; i < rows; i++) {
        cout << "Row " << i << " | ";
        int cols = min((int)table[i].size(), 10);
        for (int j = 0; j < cols; j++) {
            cout << setw(8) << fixed << setprecision(3) << table[i][j] << " ";
        }
        cout << "\n";
    }
    cout << "--------------------------------------------------\n";
}

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    double Lx = 100;
    double Lt = 0.025;
    int Nx = 50000;
    double dx = Lx / Nx;
    double r = 0.5;
    double dt = 0.5 * (dx * dx);
    double Nt = Lt / dt;

    if (r > 0.5) {
        cerr << "Error: Stability condition not met (r <= 0.5)." << endl;
        MPI_Abort(MPI_COMM_WORLD, 1);
    }

    int total_points = Nx + 1;
    int base = total_points / size;
    int rem = total_points % size;
    int local_N;
    int local_start;
    if (rank < rem) {
        local_N = base + 1;
        local_start = rank * local_N;
    }
    else {
        local_N = base;
        local_start = rank * local_N + rem;
    }
    int local_end = local_start + local_N - 1;

    cout << "Process " << rank << " is responsible for global grid points from "
        << local_start << " to " << local_end << endl;
    cout.flush();

    vector<vector<double>> u_local(Nt + 1, vector<double>(local_N + 2, 0.0));

    for (int j = 1; j <= local_N; j++) {
        int global_j = local_start + j - 1;
        double x = global_j * dx;
        u_local[0][j] = x * (1 - x);
    }

    if (local_start == 0) {
        u_local[0][1] = 0.0;
    }
    if (local_end == Nx) {
        u_local[0][local_N] = 0.0;
    }

    MPI_Barrier(MPI_COMM_WORLD);
    auto start_time = high_resolution_clock::now();

    for (int i = 1; i <= Nt; i++) {
        if (rank > 0) {
            double send_val = u_local[i - 1][1];
            double recv_val;
            MPI_Sendrecv(&send_val, 1, MPI_DOUBLE, rank - 1, 0,
                &recv_val, 1, MPI_DOUBLE, rank - 1, 1,
                MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            u_local[i - 1][0] = recv_val;
        }

        if (rank < size - 1) {
            double send_val = u_local[i - 1][local_N];
            double recv_val;
            MPI_Sendrecv(&send_val, 1, MPI_DOUBLE, rank + 1, 1,
                &recv_val, 1, MPI_DOUBLE, rank + 1, 0,
                MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            u_local[i - 1][local_N + 1] = recv_val;
        }

        for (int j = 1; j <= local_N; j++) {
            int global_j = local_start + j - 1;
            if (global_j == 0 || global_j == Nx) {
                u_local[i][j] = 0.0;
            }
            else {
                u_local[i][j] = u_local[i - 1][j] +
                    r * (u_local[i - 1][j - 1] - 2 * u_local[i - 1][j] + u_local[i - 1][j + 1]);
            }
        }
    }

    auto end_time = high_resolution_clock::now();
    duration<double> local_elapsed = end_time - start_time;
    double local_time = local_elapsed.count();
    double global_time;

    MPI_Reduce(&local_time, &global_time, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        cout << "Processes: " << size << ", Time: " << global_time << " seconds" << endl;

        cout << "Total Computation Time (sum over all processes): " << global_time << " seconds" << endl;
    }

    vector<vector<double>> exact_local(Nt + 1, vector<double>(local_N + 2, 0.0));
    vector<vector<double>> error_local(Nt + 1, vector<double>(local_N + 2, 0.0));
    for (int i = 0; i <= Nt; i++) {
        for (int j = 1; j <= local_N; j++) {
            int global_j = local_start + j - 1;
            double x = global_j * dx;
            double t = i * dt;
            double ex = exact_solution(x, t);
            exact_local[i][j] = ex;
            error_local[i][j] = u_local[i][j] - ex;
        }
    }

    vector<vector<double>> u_global, exact_global, error_global;
    if (rank == 0) {
        u_global.resize(Nt + 1, vector<double>(total_points, 0.0));
        exact_global.resize(Nt + 1, vector<double>(total_points, 0.0));
        error_global.resize(Nt + 1, vector<double>(total_points, 0.0));
    }

    vector<int> recvcounts(size), displs(size);
    int offset = 0;
    for (int p = 0; p < size; p++) {
        int points_p = (p < rem ? base + 1 : base);
        recvcounts[p] = points_p;
        displs[p] = offset;
        offset += points_p;
    }

    vector<double> send_buffer(local_N);
    for (int i = 0; i <= Nt; i++) {
        for (int j = 0; j < local_N; j++) {
            send_buffer[j] = u_local[i][j + 1];
        }
        MPI_Gatherv(send_buffer.data(), local_N, MPI_DOUBLE,
            (rank == 0 ? u_global[i].data() : nullptr),
            recvcounts.data(), displs.data(), MPI_DOUBLE,
            0, MPI_COMM_WORLD);

        for (int j = 0; j < local_N; j++) {
            send_buffer[j] = exact_local[i][j + 1];
        }
        MPI_Gatherv(send_buffer.data(), local_N, MPI_DOUBLE,
            (rank == 0 ? exact_global[i].data() : nullptr),
            recvcounts.data(), displs.data(), MPI_DOUBLE,
            0, MPI_COMM_WORLD);

        for (int j = 0; j < local_N; j++) {
            send_buffer[j] = error_local[i][j + 1];
        }
        MPI_Gatherv(send_buffer.data(), local_N, MPI_DOUBLE,
            (rank == 0 ? error_global[i].data() : nullptr),
            recvcounts.data(), displs.data(), MPI_DOUBLE,
            0, MPI_COMM_WORLD);
    }

    MPI_Barrier(MPI_COMM_WORLD);
    if (rank == 0) {
        print_table(u_global, "Numerical Solution", rank);
        print_table(exact_global, "Exact Solution", rank);
        print_table(error_global, "Error", rank);
    }

    MPI_Finalize();
    return 0;
}
